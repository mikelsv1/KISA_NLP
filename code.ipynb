{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <h1><b>Scattertext applied to Hyperpartisan News Detection</b></h1>\n",
    "    <h3><b>Authors:</b> Julen Rodriguez and Mikel Salvoch</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy under 2.x could be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scattertext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scattertext as st\n",
    "import spacy\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data\n",
    "Assemble the data to analyze into a Pandas data frame. It should have at least two columns, the text you'd like to analyze, and the category to study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hyperpartisan\\articles-training-byarticle-20181122.tsv\n",
    "convention_df = pd.read_csv('articles-training-byarticle-20181122.tsv', sep='\\t')\n",
    "# non header, first column: 1 for hyperpartisan, 0 for non-hyperpartisan\n",
    "convention_df.columns = ['hyperpartisan', 'article']\n",
    "convention_df['hyperpartisan'] = convention_df['hyperpartisan'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperpartisan</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>Trump Just Woke Up &amp; Viciously Attacked Puerto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>Liberals wailing about gun control, but what a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>Laremy Tunsil joins NFL players in kneeling du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>It's 1968 All Over Again  Almost a half-centur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>Gold Price in December 2017 - Myriads of Signa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hyperpartisan                                            article\n",
       "0           True  Trump Just Woke Up & Viciously Attacked Puerto...\n",
       "1           True  Liberals wailing about gun control, but what a...\n",
       "2           True  Laremy Tunsil joins NFL players in kneeling du...\n",
       "3          False  It's 1968 All Over Again  Almost a half-centur...\n",
       "4           True  Gold Price in December 2017 - Myriads of Signa..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convention_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load data into Scattertext corpus\n",
    "Turn the data frame into a Scattertext Corpus to begin analyzing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn it into a Scattertext Corpus \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "corpus = st.CorpusFromPandas(convention_df, \n",
    "                             category_col='hyperpartisan',\n",
    "                             text_col='article',\n",
    "                             nlp=nlp).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the terms that differentiate the corpus from a general English corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trump', 'twitter', 'obama', 'comey', 'tweeted', 'bannon', 'facebook', 'barack', 'hillary', 'kaepernick']\n"
     ]
    }
   ],
   "source": [
    "print(list(corpus.get_scaled_f_scores_vs_background().index[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the terms that are most associated with Hyperpartisan news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the left',\n",
      " 'class',\n",
      " 'israel',\n",
      " 'ruling',\n",
      " 'china',\n",
      " 'conservative',\n",
      " 'ruling class',\n",
      " 'the ruling',\n",
      " 'he ’s',\n",
      " 'the media']\n"
     ]
    }
   ],
   "source": [
    "term_freq_df = corpus.get_term_freq_df()\n",
    "term_freq_df['hyperpartisan Score'] = corpus.get_scaled_f_scores('True')\n",
    "pprint(list(term_freq_df.sort_values(by='hyperpartisan Score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the terms that are most associated with non-Hyperpartisan news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['send free',\n",
      " '|',\n",
      " '⚪',\n",
      " 'california ca',\n",
      " '⠀',\n",
      " 'hurricane',\n",
      " 'august',\n",
      " 'angeles',\n",
      " 'los angeles',\n",
      " 'los']\n"
     ]
    }
   ],
   "source": [
    "term_freq_df['non-hyperpartisan Score'] = corpus.get_scaled_f_scores('False')\n",
    "pprint(list(term_freq_df.sort_values(by='non-hyperpartisan Score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Visualizing term associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category='True',\n",
    "    category_name='Hyperpartisan',\n",
    "    not_category_name='Non-Hyperpartisan',\n",
    "    width_in_pixels=1000,\n",
    "    metadata=convention_df['hyperpartisan']\n",
    ")\n",
    "with open(\"Convention-Visualization.html\", 'wb') as file:\n",
    "    file.write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Visualizing Phrase associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytextrank\n",
      "  Downloading pytextrank-3.3.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting GitPython>=3.1 (from pytextrank)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: graphviz>=0.13 in d:\\anaconda\\lib\\site-packages (from pytextrank) (0.20.1)\n",
      "Collecting icecream>=2.1 (from pytextrank)\n",
      "  Downloading icecream-2.1.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: networkx>=2.6 in d:\\anaconda\\lib\\site-packages (from networkx[default]>=2.6->pytextrank) (3.2.1)\n",
      "Requirement already satisfied: pygments>=2.7.4 in d:\\anaconda\\lib\\site-packages (from pytextrank) (2.15.1)\n",
      "Requirement already satisfied: scipy>=1.7 in d:\\anaconda\\lib\\site-packages (from pytextrank) (1.13.1)\n",
      "Requirement already satisfied: spacy>=3.0 in d:\\anaconda\\lib\\site-packages (from pytextrank) (3.8.3)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython>=3.1->pytextrank)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: colorama>=0.3.9 in d:\\anaconda\\lib\\site-packages (from icecream>=2.1->pytextrank) (0.4.6)\n",
      "Collecting executing>=2.1.0 (from icecream>=2.1->pytextrank)\n",
      "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in d:\\anaconda\\lib\\site-packages (from icecream>=2.1->pytextrank) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.22 in d:\\anaconda\\lib\\site-packages (from networkx[default]>=2.6->pytextrank) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.5 in d:\\anaconda\\lib\\site-packages (from networkx[default]>=2.6->pytextrank) (3.9.2)\n",
      "Requirement already satisfied: pandas>=1.4 in d:\\anaconda\\lib\\site-packages (from networkx[default]>=2.6->pytextrank) (2.2.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.1.4)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda\\lib\\site-packages (from spacy>=3.0->pytextrank) (3.5.0)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.16.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=3.1->pytextrank)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\anaconda\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0->pytextrank) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.5->networkx[default]>=2.6->pytextrank) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.5->networkx[default]>=2.6->pytextrank) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.5->networkx[default]>=2.6->pytextrank) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.5->networkx[default]>=2.6->pytextrank) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.5->networkx[default]>=2.6->pytextrank) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.5->networkx[default]>=2.6->pytextrank) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.5->networkx[default]>=2.6->pytextrank) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.5->networkx[default]>=2.6->pytextrank) (6.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas>=1.4->networkx[default]>=2.6->pytextrank) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas>=1.4->networkx[default]>=2.6->pytextrank) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in d:\\anaconda\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy>=3.0->pytextrank) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\anaconda\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy>=3.0->pytextrank) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\anaconda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->pytextrank) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\anaconda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0->pytextrank) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\anaconda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0->pytextrank) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->spacy>=3.0->pytextrank) (2.1.3)\n",
      "Collecting numpy>=1.22 (from networkx[default]>=2.6->pytextrank)\n",
      "  Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\anaconda\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.5->networkx[default]>=2.6->pytextrank) (3.21.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\anaconda\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0->pytextrank) (1.2.1)\n",
      "Requirement already satisfied: wrapt in d:\\anaconda\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0->pytextrank) (1.17.0)\n",
      "Downloading pytextrank-3.3.0-py3-none-any.whl (26 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading icecream-2.1.4-py3-none-any.whl (14 kB)\n",
      "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, numpy, executing, icecream, gitdb, GitPython, pytextrank\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (d:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (d:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (d:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (d:\\anaconda\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'd:\\\\anaconda\\\\lib\\\\site-packages\\\\numpy\\\\_core\\\\_multiarray_tests.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install --user pytextrank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperpartisan</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>Trump Just Woke Up &amp; Viciously Attacked Puerto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>Liberals wailing about gun control, but what a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>Laremy Tunsil joins NFL players in kneeling du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>It's 1968 All Over Again  Almost a half-centur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>Gold Price in December 2017 - Myriads of Signa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hyperpartisan                                            article\n",
       "0           True  Trump Just Woke Up & Viciously Attacked Puerto...\n",
       "1           True  Liberals wailing about gun control, but what a...\n",
       "2           True  Laremy Tunsil joins NFL players in kneeling du...\n",
       "3          False  It's 1968 All Over Again  Almost a half-centur...\n",
       "4           True  Gold Price in December 2017 - Myriads of Signa..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convention_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pytextrank' has no attribute 'TextRank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\scattertext\\features\\PyTextRankPhrases.py:34\u001b[0m, in \u001b[0;36mPyTextRankPhrases.get_doc_metadata\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphrases\u001b[49m:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_include_chunks:\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\spacy\\tokens\\underscore.py:48\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE046\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m     49\u001b[0m default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'phrases'. Did you forget to call the `set_extension` method?",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# nlp.add_pipe(\"textrank\", last=True)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m convention_df \u001b[38;5;241m=\u001b[39m convention_df\u001b[38;5;241m.\u001b[39massign(\n\u001b[0;32m      6\u001b[0m     parse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m df: df\u001b[38;5;241m.\u001b[39marticle\u001b[38;5;241m.\u001b[39mapply(nlp),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# we label as hyperpartisan and non_hyperpartisan to avoid problems with True and False reserved words\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     hyperpartisan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m df: df\u001b[38;5;241m.\u001b[39mhyperpartisan\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperpartisan\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon_hyperpartisan\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCorpusFromParsedDocuments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvention_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyperpartisan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparsed_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeats_from_spacy_doc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTextRankPhrases\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcompact(\n\u001b[0;32m     17\u001b[0m     st\u001b[38;5;241m.\u001b[39mAssociationCompactor(\u001b[38;5;241m2000\u001b[39m, use_non_text_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\scattertext\\CorpusFromParsedDocuments.py:48\u001b[0m, in \u001b[0;36mCorpusFromParsedDocuments.build\u001b[1;34m(self, show_progress)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_to_x_factory, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_to_x_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mX \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mX_factory\u001b[38;5;241m.\u001b[39mset_last_row_idx(\u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mget_csr_matrix()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ParsedCorpus(\n\u001b[0;32m     51\u001b[0m     df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df,\n\u001b[0;32m     52\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X_factory\u001b[38;5;241m.\u001b[39mset_last_row_idx(\u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mget_csr_matrix(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     category_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_category_col\n\u001b[0;32m     60\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\scattertext\\CorpusFromParsedDocuments.py:70\u001b[0m, in \u001b[0;36mCorpusFromParsedDocuments._add_to_x_factory\u001b[1;34m(self, row)\u001b[0m\n\u001b[0;32m     68\u001b[0m     term_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_term_idx_store\u001b[38;5;241m.\u001b[39mgetidx(term)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_X_factory[row\u001b[38;5;241m.\u001b[39mname, term_idx] \u001b[38;5;241m=\u001b[39m count\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m meta, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feats_from_spacy_doc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_doc_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed_text\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     71\u001b[0m     meta_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_idx_store\u001b[38;5;241m.\u001b[39mgetidx(meta)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mX_factory[row\u001b[38;5;241m.\u001b[39mname, meta_idx] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\scattertext\\features\\PyTextRankPhrases.py:42\u001b[0m, in \u001b[0;36mPyTextRankPhrases.get_doc_metadata\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m: \u001b[38;5;66;03m# Support for pytextrank<3\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytextrank\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     tr \u001b[38;5;241m=\u001b[39m \u001b[43mpytextrank\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextRank\u001b[49m()\n\u001b[0;32m     43\u001b[0m     tr\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m doc\n\u001b[0;32m     44\u001b[0m     phrases \u001b[38;5;241m=\u001b[39m tr\u001b[38;5;241m.\u001b[39mcalc_textrank()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pytextrank' has no attribute 'TextRank'"
     ]
    }
   ],
   "source": [
    "import pytextrank\n",
    "\n",
    "# nlp.add_pipe(\"textrank\", last=True)\n",
    "\n",
    "convention_df = convention_df.assign(\n",
    "    parse=lambda df: df.article.apply(nlp),\n",
    "    # we label as hyperpartisan and non_hyperpartisan to avoid problems with True and False reserved words\n",
    "    hyperpartisan=lambda df: df.hyperpartisan.apply(lambda x: 'hyperpartisan' if x else 'non_hyperpartisan')\n",
    ")\n",
    "corpus = st.CorpusFromParsedDocuments(\n",
    "    convention_df,\n",
    "    category_col='hyperpartisan',\n",
    "    parsed_col='parse',\n",
    "    feats_from_spacy_doc=st.PyTextRankPhrases()\n",
    ").build(\n",
    ").compact(\n",
    "    st.AssociationCompactor(2000, use_non_text_features=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the terms present in the corpus are named entities, and, as opposed to frequency counts, their scores are the eigencentrality scores assigned to them by the TextRank algorithm. Running ```corpus.get_metadata_freq_df('')``` will return, for each category, the sums of terms' TextRank scores. The dense ranks of these scores will be used to construct the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            hyperpartisan  non_hyperpartisan\n",
      "term                                                        \n",
      "Puerto Rican people              0.104552           0.000000\n",
      "Donald Trump                    22.772139          24.630074\n",
      "Puerto Rican                     0.468275           0.000000\n",
      "Trump                          125.671835          88.974905\n",
      "Puerto Rico                      3.506832           0.511172\n",
      "...                                   ...                ...\n",
      "the stomach                      0.013283           0.000000\n",
      "another interview                0.013148           0.000000\n",
      "his depravity                    0.012987           0.000000\n",
      "F**                              0.009696           0.000000\n",
      "And possibly tranquilizers       0.005146           0.000000\n",
      "\n",
      "[49258 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "term_category_scores = corpus.get_metadata_freq_df('')\n",
    "print(term_category_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we construct the plot, let's some helper variables Since the aggregate TextRank scores aren't particularly interpretable, we'll display the per-category rank of each score in the metadata_description field. These will be displayed after a term is clicked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ranks = pd.DataFrame(\n",
    "    np.argsort(np.argsort(-term_category_scores, axis=0), axis=0) + 1,\n",
    "    columns=term_category_scores.columns,\n",
    "    index=term_category_scores.index)\n",
    "\n",
    "metadata_descriptions = {\n",
    "    term: '<br/>' + '<br/>'.join(\n",
    "        '<b>%s</b> TextRank score rank: %s/%s' % (cat, term_ranks.loc[term, cat], corpus.get_num_metadata())\n",
    "        for cat in corpus.get_categories())\n",
    "    for term in corpus.get_metadata()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_specific_prominence = term_category_scores.apply(\n",
    "    lambda r: r.hyperpartisan if r.hyperpartisan > r.non_hyperpartisan else -r.non_hyperpartisan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to construct the plot defining an html file to save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category='hyperpartisan',\n",
    "    not_category_name='non_hyperpartisan',\n",
    "    minimum_term_frequency=0,\n",
    "    pmi_threshold_coefficient=0,\n",
    "    width_in_pixels=1000,\n",
    "    transform=st.dense_rank,\n",
    "    metadata=corpus.get_df()['hyperpartisan'],\n",
    "    scores=category_specific_prominence,\n",
    "    sort_by_dist=False,\n",
    "    use_non_text_features=True,\n",
    "    topic_model_term_lists={term: [term] for term in corpus.get_metadata()},\n",
    "    topic_model_preview_size=0,\n",
    "    metadata_descriptions=metadata_descriptions,\n",
    "    use_full_doc=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Convention-Visualization-TextRank.html\", 'wb') as file:\n",
    "    file.write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Visualizing Empath topics and categories\n",
    "\n",
    "In order to visualize Empath (Fast et al., 2016) topics and categories instead of terms, we'll need to create a Corpus of extracted topics and categories rather than unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperpartisan</th>\n",
       "      <th>article</th>\n",
       "      <th>parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>Trump Just Woke Up &amp; Viciously Attacked Puerto...</td>\n",
       "      <td>(Trump, Just, Woke, Up, &amp;, Viciously, Attacked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>Liberals wailing about gun control, but what a...</td>\n",
       "      <td>(Liberals, wailing, about, gun, control, ,, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>Laremy Tunsil joins NFL players in kneeling du...</td>\n",
       "      <td>(Laremy, Tunsil, joins, NFL, players, in, knee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non_hyperpartisan</td>\n",
       "      <td>It's 1968 All Over Again  Almost a half-centur...</td>\n",
       "      <td>(It, 's, 1968, All, Over, Again,  , Almost, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>Gold Price in December 2017 - Myriads of Signa...</td>\n",
       "      <td>(Gold, Price, in, December, 2017, -, Myriads, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hyperpartisan                                            article  \\\n",
       "0      hyperpartisan  Trump Just Woke Up & Viciously Attacked Puerto...   \n",
       "1      hyperpartisan  Liberals wailing about gun control, but what a...   \n",
       "2      hyperpartisan  Laremy Tunsil joins NFL players in kneeling du...   \n",
       "3  non_hyperpartisan  It's 1968 All Over Again  Almost a half-centur...   \n",
       "4      hyperpartisan  Gold Price in December 2017 - Myriads of Signa...   \n",
       "\n",
       "                                               parse  \n",
       "0  (Trump, Just, Woke, Up, &, Viciously, Attacked...  \n",
       "1  (Liberals, wailing, about, gun, control, ,, bu...  \n",
       "2  (Laremy, Tunsil, joins, NFL, players, in, knee...  \n",
       "3  (It, 's, 1968, All, Over, Again,  , Almost, a,...  \n",
       "4  (Gold, Price, in, December, 2017, -, Myriads, ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convention_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_builder = st.FeatsFromOnlyEmpath()\n",
    "empath_corpus = st.CorpusFromParsedDocuments(convention_df,\n",
    "                                             category_col='hyperpartisan',\n",
    "                                             feats_from_spacy_doc=feat_builder,\n",
    "                                             parsed_col='parse').build()\n",
    "html = st.produce_scattertext_explorer(empath_corpus,\n",
    "                                       category='hyperpartisan',\n",
    "                                       category_name='Hyperpartisan',\n",
    "                                       not_category_name='Non-Hyperpartisan',\n",
    "                                       width_in_pixels=1000,\n",
    "                                       metadata=convention_df['hyperpartisan'],\n",
    "                                       use_non_text_features=True,\n",
    "                                       use_full_doc=True,\n",
    "                                       topic_model_term_lists=feat_builder.get_top_model_term_lists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Convention-Visualization-Empath.html\", 'wb') as file:\n",
    "    file.write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Ordering Terms by Corpus Characteristicness\n",
    "\n",
    "We are identifying terms that are frequent within the studied documents but less common in general language. The characteristic score compares these terms against a general English frequency list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***CAUTION: run only if needed***\n",
    "convention_df = convention_df.assign(\n",
    "    parse=lambda df: df.article.apply(nlp),\n",
    "    # we label as hyperpartisan and non_hyperpartisan to avoid problems with True and False reserved words\n",
    "    hyperpartisan=lambda df: df.hyperpartisan.apply(lambda x: 'hyperpartisan' if x else 'non_hyperpartisan')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperpartisan</th>\n",
       "      <th>article</th>\n",
       "      <th>parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>Trump Just Woke Up &amp; Viciously Attacked Puerto...</td>\n",
       "      <td>(Trump, Just, Woke, Up, &amp;, Viciously, Attacked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>Liberals wailing about gun control, but what a...</td>\n",
       "      <td>(Liberals, wailing, about, gun, control, ,, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>Laremy Tunsil joins NFL players in kneeling du...</td>\n",
       "      <td>(Laremy, Tunsil, joins, NFL, players, in, knee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non_hyperpartisan</td>\n",
       "      <td>It's 1968 All Over Again  Almost a half-centur...</td>\n",
       "      <td>(It, 's, 1968, All, Over, Again,  , Almost, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>Gold Price in December 2017 - Myriads of Signa...</td>\n",
       "      <td>(Gold, Price, in, December, 2017, -, Myriads, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       hyperpartisan                                            article  \\\n",
       "0      hyperpartisan  Trump Just Woke Up & Viciously Attacked Puerto...   \n",
       "1      hyperpartisan  Liberals wailing about gun control, but what a...   \n",
       "2      hyperpartisan  Laremy Tunsil joins NFL players in kneeling du...   \n",
       "3  non_hyperpartisan  It's 1968 All Over Again  Almost a half-centur...   \n",
       "4      hyperpartisan  Gold Price in December 2017 - Myriads of Signa...   \n",
       "\n",
       "                                               parse  \n",
       "0  (Trump, Just, Woke, Up, &, Viciously, Attacked...  \n",
       "1  (Liberals, wailing, about, gun, control, ,, bu...  \n",
       "2  (Laremy, Tunsil, joins, NFL, players, in, knee...  \n",
       "3  (It, 's, 1968, All, Over, Again,  , Almost, a,...  \n",
       "4  (Gold, Price, in, December, 2017, -, Myriads, ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convention_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = (st.CorpusFromPandas(convention_df,\n",
    "                              category_col='hyperpartisan',\n",
    "                              text_col='article',\n",
    "                              nlp=st.whitespace_nlp_with_sentences)\n",
    "          .build()\n",
    "          .get_unigram_corpus()\n",
    "          .compact(st.ClassPercentageCompactor(term_count=2,\n",
    "                                               term_ranker=st.OncePerDocFrequencyRanker)))\n",
    "html = st.produce_characteristic_explorer(\n",
    "    corpus,\n",
    "    category='hyperpartisan',\n",
    "    category_name='Hyperpartisan',\n",
    "    not_category_name='Non-Hyperpartisan',\n",
    "    metadata=convention_df['hyperpartisan'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Convention-Visualization-Characteristics.html\", 'wb') as file:\n",
    "    file.write(html.encode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
